# Laughter Detection Model ğŸ­ğŸ˜‚

A deep learning-based system for detecting and segmenting laughter in audio recordings using ResNet architectures and advanced audio feature extraction techniques.

## ğŸ¯ Overview

This project implements an end-to-end audio classification pipeline that automatically detects laughter segments in speech recordings. The system uses convolutional neural networks (ResNet) trained on mel-spectrogram features to identify laughter with precise temporal boundaries.

**Key Features:**
- ğŸ§  ResNet-based deep learning architecture with residual blocks
- ğŸµ Advanced audio feature extraction (mel-spectrograms, MFCC)
- ğŸ”„ Data augmentation pipeline (SpecAugment, waveform perturbations)
- ğŸ“Š Trained on AudioSet and Switchboard conversational datasets
- â±ï¸ Temporal segmentation with configurable sensitivity thresholds
- ğŸ“ˆ Comprehensive evaluation metrics (precision, recall, F1-score)

## ğŸ—ï¸ Architecture

### Model Configurations

The project supports multiple model architectures:

1. **MLP Baseline (MFCC)** - Multi-layer perceptron using MFCC features
2. **ResNet Base** - ResNet without augmentation for baseline comparison
3. **ResNet with Augmentation** - Enhanced ResNet with SpecAugment and waveform augmentation (recommended)

### Network Architecture

```
Input Audio (8kHz) 
    â†“
Mel-Spectrogram Feature Extraction (hop_length=186, 100 FPS)
    â†“
ResNet Architecture:
  - Conv2D (64 filters, 3x3 kernel)
  - Batch Normalization
  - 4 Residual Blocks [128â†’64â†’32â†’32 filters]
  - Global Average Pooling
  - Fully Connected Layers (Linear â†’ Dropout â†’ Sigmoid)
    â†“
Binary Classification (Laughter / Non-Laughter)
    â†“
Temporal Segmentation (Lowpass Filter â†’ Threshold â†’ Min Duration)
    â†“
Output: [(start_time, end_time), ...]
```

## ğŸ“¦ Installation

### Prerequisites

- Python 3.7+
- CUDA-capable GPU (optional but recommended)

### Setup

```bash
# Clone the repository
git clone https://github.com/s-cube-15/laughter-detection-model.git
cd laughter-detection-model

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Dependencies

```
torch==1.7.1
librosa==0.8.1
tgt==1.4.4
pyloudnorm==0.1.0
praatio==3.8.0
tensorboardX==1.9
pandas
numpy==1.21.5
scikit-learn==1.0.2
streamlit==1.7.0
```

## ğŸš€ Usage

### 1. Inference: Detect Laughter in Audio Files

```bash
python segment_laughter.py \
    --input_audio_file=path/to/audio.wav \
    --output_dir=./output \
    --threshold=0.5 \
    --min_length=0.2 \
    --save_to_audio_files=True \
    --save_to_textgrid=False
```

**Parameters:**
- `--input_audio_file`: Path to input audio file (WAV format)
- `--output_dir`: Directory to save output files
- `--threshold`: Detection threshold (0.0-1.0, default: 0.5)
- `--min_length`: Minimum laughter duration in seconds (default: 0.2)
- `--save_to_audio_files`: Extract laughter segments as separate audio files
- `--save_to_textgrid`: Save results in Praat TextGrid format

### 2. Training a Model

```bash
# Train ResNet with augmentation
python train.py \
    --config=resnet_with_augmentation \
    --batch_size=32 \
    --checkpoint_dir=./checkpoints/my_model

# Train on AudioSet (noisy data)
python train.py \
    --config=resnet_with_augmentation \
    --batch_size=32 \
    --checkpoint_dir=./checkpoints/audioset_model \
    --train_on_noisy_audioset=True
```

### 3. Streamlit Web Application

```bash
streamlit run app.py
```

Open your browser and navigate to `http://localhost:8501` to use the interactive web interface.

## ğŸ“Š Dataset

The model is trained on two primary datasets:

### 1. **AudioSet**
- Large-scale audio event dataset
- Contains diverse laughter samples from various contexts
- Includes noisy real-world recordings

### 2. **Switchboard Corpus**
- Telephonic conversational speech
- High-quality annotations
- Used for validation and testing

Dataset structure:
```
data/
â”œâ”€â”€ audioset/
â”‚   â”œâ”€â”€ annotations/
â”‚   â”œâ”€â”€ splits/
â”‚   â””â”€â”€ AI_open_mic_dataset/
â””â”€â”€ switchboard/
    â”œâ”€â”€ train/
    â”œâ”€â”€ val/
    â””â”€â”€ test/
```

## ğŸ“ Model Training Details

### Feature Extraction
- **Sampling Rate:** 8000 Hz
- **Feature Type:** Mel-spectrograms (default) or MFCC
- **Hop Length:** 186 samples
- **Frame Rate:** 100 FPS

### Data Augmentation
- **SpecAugment:** Time and frequency masking
- **Waveform Augmentation:** Time-stretching, pitch-shifting
- **Loudness Normalization:** Using pyloudnorm

### Training Configuration
- **Optimizer:** SGD with momentum
- **Learning Rate:** 0.01 (decay: 0.9999)
- **Batch Size:** 32
- **Dropout Rate:** 0.5
- **Training Steps:** 100,000
- **Device:** CUDA (GPU) or CPU

## ğŸ“ˆ Evaluation

The project includes comprehensive evaluation scripts:

```bash
# Evaluate on Switchboard test set
python scripts/Evaluation/evaluate_resnet_specaug_wavaug_on_switchboard.py

# Evaluate on AudioSet
python scripts/Evaluation/evaluate_resnet_specaug_wavaug_on_audioset.py

# Analyze results with bootstrap confidence intervals
python scripts/Evaluation/analyze_results.py
```

### Metrics
- **Frame-level Accuracy:** Percentage of correctly classified frames
- **Precision:** True positive rate among predicted laughter
- **Recall:** True positive rate among actual laughter
- **F1-Score:** Harmonic mean of precision and recall
- **Event-based Metrics:** Detection of laughter events (start/end)

## ğŸ—‚ï¸ Project Structure

```
laughter-detection-model/
â”œâ”€â”€ app.py                      # Streamlit web application
â”œâ”€â”€ segment_laughter.py         # Inference script
â”œâ”€â”€ train.py                    # Training script
â”œâ”€â”€ model.py                    # Main model notebook
â”œâ”€â”€ models.py                   # Neural network architectures
â”œâ”€â”€ configs.py                  # Model configurations
â”œâ”€â”€ laugh_segmenter.py          # Segmentation utilities
â”œâ”€â”€ rating_humour_quotient.py   # Humor rating module
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ in_use/                 # Active model checkpoints
â”‚   â””â”€â”€ comparisons/            # Baseline comparisons
â”œâ”€â”€ data/                       # Training/validation data
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ Evaluation/             # Evaluation scripts
â”‚   â”œâ”€â”€ download_audioset_*.py  # Data download utilities
â”‚   â””â”€â”€ aggregate_*.py          # Annotation processing
â””â”€â”€ utils/
    â”œâ”€â”€ audio_utils.py          # Audio processing functions
    â”œâ”€â”€ data_loaders.py         # PyTorch data loaders
    â”œâ”€â”€ dataset_utils.py        # Dataset utilities
    â”œâ”€â”€ torch_utils.py          # PyTorch helpers
    â””â”€â”€ text_utils.py           # Text processing
```

## ğŸ”¬ Technical Details

### Temporal Segmentation Algorithm

1. **Frame-level Classification:** Model predicts laughter probability for each audio frame (100 FPS)
2. **Lowpass Filtering:** Butterworth filter smooths probability curve
3. **Thresholding:** Frames above threshold are marked as laughter
4. **Boundary Detection:** Consecutive laughter frames are grouped
5. **Duration Filtering:** Segments shorter than minimum duration are discarded
6. **Time Conversion:** Frame indices converted to seconds

### Preprocessing Pipeline

```python
Audio File (any format)
    â†“
Load with Librosa (resample to 8kHz)
    â†“
Extract Mel-Spectrogram Features
    â†“
Apply Augmentation (training only)
    â†“
Normalize and Pad Sequences
    â†“
Convert to PyTorch Tensor
    â†“
Feed to Model
```

## ğŸ¯ Use Cases

- **Comedy Analysis:** Analyze stand-up comedy performances
- **Meeting Analytics:** Detect engagement in video conferences
- **Content Moderation:** Identify laugh tracks in media
- **Research:** Study social dynamics and humor patterns
- **Accessibility:** Generate laugh captions for hearing-impaired users
- **Entertainment:** Interactive humor rating systems

## ğŸ“ Example Output

```python
[
    {'start': 2.34, 'end': 3.12},
    {'start': 5.67, 'end': 6.89},
    {'start': 12.45, 'end': 14.23}
]
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- AudioSet dataset by Google Research
- Switchboard Corpus by LDC
- PyTorch and Librosa communities
- ResNet architecture inspiration

## ğŸ“§ Contact

**Sudhanshu Sabale**
- Email: sudhanshussable2@gmail.com
- GitHub: [@s-cube-15](https://github.com/s-cube-15)
- LinkedIn: [Sudhanshu Sabale](https://www.linkedin.com/in/sudhanshu-sabale-28ab4520a/)

## ğŸŒŸ Citation

If you use this project in your research, please cite:

```bibtex
@software{sabale2024laughter,
  author = {Sabale, Sudhanshu},
  title = {Laughter Detection Model},
  year = {2024},
  url = {https://github.com/s-cube-15/laughter-detection-model}
}
```

---

â­ **Star this repository if you find it helpful!**
